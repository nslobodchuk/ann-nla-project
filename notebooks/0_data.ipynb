{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8ee1d1-499f-410d-aa49-0b707e838965",
   "metadata": {},
   "source": [
    "# Data \n",
    "\n",
    "Download from here: https://nlp.stanford.edu/data/glove.6B.zip (https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "Unzip it into `../artifacts/glove.6B/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247228c7-c28c-4fd1-9fee-56c6189a481d",
   "metadata": {},
   "source": [
    "# GloVe ANN benchmark (base / queries / ground truth)\n",
    "\n",
    "This notebook loads Stanford GloVe embeddings and constructs a simple ANN benchmark for cosine (angular) similarity.\n",
    "\n",
    "- **Parsing:** the file `glove.6B.*d.txt` is parsed into a token list `words` and an embedding matrix $E \\in \\mathbb{R}^{M \\times D}$, where each row corresponds to a word vector.\n",
    "- **Base and queries:** the search base is defined as the first $N$ vectors $X \\in \\mathbb{R}^{N \\times D}$, and the query set is formed by sampling a random subset of rows from $X$ using indices $q_{\\text{idx}}$: $Q = X[q_{\\text{idx}}] \\in \\mathbb{R}^{n_q \\times D}$.\n",
    "- **Cosine / angular setting:** vectors are L2-normalized $X_n = \\frac{X}{\\|X\\|_2}$ and $Q_n = \\frac{Q}{\\|Q\\|_2}$, so cosine similarity reduces to a dot product $s(q, x) = \\frac{q^\\top x}{\\|q\\|_2\\|x\\|_2} = q_n^\\top x_n$.\n",
    "- **Ground truth (exact top-$k$):** exact neighbors are computed via batched matrix multiplication $S = Q_n X_n^\\top$, excluding the trivial self-match (since queries are drawn from the base). The benchmark stores $\\text{gt\\_ids} \\in \\mathbb{N}^{n_q \\times k}$ (top-$k$ neighbor indices) and $\\text{gt\\_scores} \\in \\mathbb{R}^{n_q \\times k}$ (corresponding cosine similarity values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664b80a9-2c44-43ff-b25d-fab23fc5c478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_ID: 989456bc6cc2e6ed\n",
      "\n",
      "=== Processing 50d ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f5b96551484dd19c4536a8f4254ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GT over X batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- glove50d_base_N400000__ds989456bc6cc2e6ed.npz\n",
      "- glove50d_queries_N400000_nq100000_test20000_seed42__ds989456bc6cc2e6ed.npz\n",
      "- glove50d_gt_test_N400000_nq20000_k50_seed42__ds989456bc6cc2e6ed.npz\n",
      "\n",
      "=== Processing 100d ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ed29abd2dc4d6c80b60dcbfb7dfbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GT over X batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- glove100d_base_N400000__ds989456bc6cc2e6ed.npz\n",
      "- glove100d_queries_N400000_nq100000_test20000_seed42__ds989456bc6cc2e6ed.npz\n",
      "- glove100d_gt_test_N400000_nq20000_k50_seed42__ds989456bc6cc2e6ed.npz\n",
      "\n",
      "=== Processing 200d ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0898e9d551b49d682d441aa65224ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GT over X batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- glove200d_base_N400000__ds989456bc6cc2e6ed.npz\n",
      "- glove200d_queries_N400000_nq100000_test20000_seed42__ds989456bc6cc2e6ed.npz\n",
      "- glove200d_gt_test_N400000_nq20000_k50_seed42__ds989456bc6cc2e6ed.npz\n",
      "\n",
      "=== Processing 300d ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261e2bf2faf44369add9c7439a7f0270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GT over X batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- glove300d_base_N400000__ds989456bc6cc2e6ed.npz\n",
      "- glove300d_queries_N400000_nq100000_test20000_seed42__ds989456bc6cc2e6ed.npz\n",
      "- glove300d_gt_test_N400000_nq20000_k50_seed42__ds989456bc6cc2e6ed.npz\n",
      "\n",
      "Done. DATASET_ID: 989456bc6cc2e6ed\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 0_data â€” Build GloVe ANN benchmark artifacts (base / queries / ground truth)\n",
    "# Produces comparable datasets across dimensions with a shared query split and a shared dataset_id.\n",
    "\n",
    "# %%\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# Paths / config\n",
    "# ----------------------------\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "ARTIFACTS = (REPO_ROOT / \"artifacts\").resolve()\n",
    "OUT_DIR = (ARTIFACTS / \"data\").resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GLOVE_DIR = (ARTIFACTS / \"glove.6B\").resolve()\n",
    "GLOVE_DIMS = [50, 100, 200, 300]\n",
    "\n",
    "N_BASE = 400_000\n",
    "NQ_TOTAL = 100_000\n",
    "NQ_TEST = 20_000\n",
    "TOP_K = 50\n",
    "SEED = 42\n",
    "\n",
    "X_BATCH = 5_000\n",
    "Q_BATCH = 1_024\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def l2_normalize(X: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / np.maximum(n, eps)\n",
    "\n",
    "def _topk_desc_1d(a: np.ndarray, k: int) -> np.ndarray:\n",
    "    k = int(k)\n",
    "    if k <= 0:\n",
    "        return np.empty((0,), dtype=np.int64)\n",
    "    if k >= a.size:\n",
    "        return np.argsort(-a)\n",
    "    idx = np.argpartition(-a, kth=k - 1)[:k]\n",
    "    return idx[np.argsort(-a[idx])]\n",
    "\n",
    "def compute_dataset_id(\n",
    "    n_base: int,\n",
    "    seed: int,\n",
    "    q_idx_all: np.ndarray,\n",
    "    q_idx_test: np.ndarray,\n",
    "    version: str = \"v1\",\n",
    ") -> str:\n",
    "    h = hashlib.sha256()\n",
    "    h.update(version.encode(\"utf-8\"))\n",
    "    h.update(str(int(n_base)).encode(\"utf-8\"))\n",
    "    h.update(str(int(seed)).encode(\"utf-8\"))\n",
    "    h.update(np.ascontiguousarray(q_idx_all, dtype=np.int32).tobytes())\n",
    "    h.update(np.ascontiguousarray(q_idx_test, dtype=np.int32).tobytes())\n",
    "    return h.hexdigest()[:16]\n",
    "\n",
    "def hash_tokens(tokens: List[str]) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    for t in tokens:\n",
    "        h.update(t.encode(\"utf-8\", errors=\"replace\"))\n",
    "        h.update(b\"\\n\")\n",
    "    return h.hexdigest()\n",
    "\n",
    "# ----------------------------\n",
    "# Fast GloVe loader\n",
    "# ----------------------------\n",
    "def load_glove_fast(path: str | Path, max_rows: Optional[int] = None) -> Tuple[List[str], np.ndarray]:\n",
    "    words: List[str] = []\n",
    "    vecs: List[np.ndarray] = []\n",
    "    dim: Optional[int] = None\n",
    "\n",
    "    path = str(path)\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_rows is not None and i >= int(max_rows):\n",
    "                break\n",
    "            parts = line.rstrip().split(\" \")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            w = parts[0]\n",
    "            v = np.asarray(parts[1:], dtype=np.float32)\n",
    "            if dim is None:\n",
    "                dim = int(v.size)\n",
    "            if v.size != dim:\n",
    "                continue\n",
    "            words.append(w)\n",
    "            vecs.append(v)\n",
    "\n",
    "    if dim is None:\n",
    "        raise RuntimeError(f\"Failed to infer dim from: {path}\")\n",
    "\n",
    "    E = np.vstack(vecs).astype(np.float32, copy=False)\n",
    "    return words, E\n",
    "\n",
    "# ----------------------------\n",
    "# Exact ground truth (cosine/IP on unit vectors)\n",
    "# ----------------------------\n",
    "def ground_truth_topk_cosine(\n",
    "    Xn: np.ndarray,\n",
    "    Qn: np.ndarray,\n",
    "    k: int,\n",
    "    exclude_ids: Optional[np.ndarray] = None,  # shape (nq,) or None\n",
    "    x_batch: int = 5_000,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Exact top-k neighbors for cosine similarity on unit-normalized vectors.\n",
    "    Returns (gt_ids, gt_scores) with shapes (nq, k).\n",
    "    \"\"\"\n",
    "    Xn = np.ascontiguousarray(Xn, dtype=np.float32)\n",
    "    Qn = np.ascontiguousarray(Qn, dtype=np.float32)\n",
    "\n",
    "    nq = int(Qn.shape[0])\n",
    "    k = int(k)\n",
    "    gt_ids = np.full((nq, k), -1, dtype=np.int32)\n",
    "    gt_sc = np.full((nq, k), -np.inf, dtype=np.float32)\n",
    "\n",
    "    for start in tqdm(range(0, Xn.shape[0], int(x_batch)), desc=\"GT over X batches\"):\n",
    "        end = min(Xn.shape[0], start + int(x_batch))\n",
    "        Xb = Xn[start:end]  # (b, d)\n",
    "\n",
    "        sims = Qn @ Xb.T  # (nq, b)\n",
    "\n",
    "        if exclude_ids is not None:\n",
    "            # Exclude self-match when queries are subset of base: if exclude_id in [start, end).\n",
    "            ex = exclude_ids.astype(np.int64, copy=False)\n",
    "            m = (ex >= start) & (ex < end)\n",
    "            if np.any(m):\n",
    "                rows = np.nonzero(m)[0]\n",
    "                cols = (ex[m] - start).astype(np.int64, copy=False)\n",
    "                sims[rows, cols] = -np.inf\n",
    "\n",
    "        # Merge current batch top-k with running top-k\n",
    "        b = sims.shape[1]\n",
    "        kk = min(k, b)\n",
    "\n",
    "        idx_local = np.argpartition(-sims, kth=kk - 1, axis=1)[:, :kk]  # (nq, kk)\n",
    "        sc_local = np.take_along_axis(sims, idx_local, axis=1)          # (nq, kk)\n",
    "        ids_local = (idx_local + start).astype(np.int32, copy=False)    # global ids\n",
    "\n",
    "        ids_merge = np.concatenate([gt_ids, ids_local], axis=1)         # (nq, k+kk)\n",
    "        sc_merge = np.concatenate([gt_sc, sc_local], axis=1)            # (nq, k+kk)\n",
    "\n",
    "        # Keep best k from merged\n",
    "        idx_keep = np.argpartition(-sc_merge, kth=k - 1, axis=1)[:, :k]  # (nq, k)\n",
    "        sc_keep = np.take_along_axis(sc_merge, idx_keep, axis=1)\n",
    "        ids_keep = np.take_along_axis(ids_merge, idx_keep, axis=1)\n",
    "\n",
    "        # Sort final k descending for cleanliness\n",
    "        order = np.argsort(-sc_keep, axis=1)\n",
    "        gt_sc = np.take_along_axis(sc_keep, order, axis=1).astype(np.float32, copy=False)\n",
    "        gt_ids = np.take_along_axis(ids_keep, order, axis=1).astype(np.int32, copy=False)\n",
    "\n",
    "    return gt_ids, gt_sc\n",
    "\n",
    "# ----------------------------\n",
    "# Shared query ids for all dims\n",
    "# ----------------------------\n",
    "rng = np.random.default_rng(SEED)\n",
    "q_idx_all = rng.choice(N_BASE, size=NQ_TOTAL, replace=False).astype(np.int32)\n",
    "\n",
    "# Deterministic split (stable across reruns)\n",
    "q_idx_test = q_idx_all[:NQ_TEST].copy()\n",
    "q_idx_train = q_idx_all[NQ_TEST:].copy()\n",
    "\n",
    "DATASET_ID = compute_dataset_id(N_BASE, SEED, q_idx_all, q_idx_test)\n",
    "print(\"DATASET_ID:\", DATASET_ID)\n",
    "\n",
    "# ----------------------------\n",
    "# Build artifacts per dim\n",
    "# ----------------------------\n",
    "master_words_hash: Optional[str] = None\n",
    "master_words_sample: Optional[list[str]] = None\n",
    "\n",
    "for d in GLOVE_DIMS:\n",
    "    print(f\"\\n=== Processing {d}d ===\")\n",
    "    glove_path = (GLOVE_DIR / f\"glove.6B.{d}d.txt\").resolve()\n",
    "    words, E = load_glove_fast(glove_path, max_rows=N_BASE)\n",
    "\n",
    "    base_words = words[:N_BASE]\n",
    "    X = E[:N_BASE].astype(np.float32, copy=False)\n",
    "    Xn = l2_normalize(X)\n",
    "\n",
    "    # Strict vocab alignment across dims\n",
    "    words_hash = hash_tokens(base_words)\n",
    "    if master_words_hash is None:\n",
    "        master_words_hash = words_hash\n",
    "        master_words_sample = [base_words[i] for i in (0, 1, 2, 10, 123, 999, N_BASE - 1)]\n",
    "    else:\n",
    "        if words_hash != master_words_hash:\n",
    "            raise RuntimeError(\n",
    "                f\"Vocabulary order mismatch across dims. \"\n",
    "                f\"Expected hash={master_words_hash}, got hash={words_hash} for dim={d}.\"\n",
    "            )\n",
    "        # cheap sanity\n",
    "        sample = [base_words[i] for i in (0, 1, 2, 10, 123, 999, N_BASE - 1)]\n",
    "        if sample != master_words_sample:\n",
    "            raise RuntimeError(f\"Vocabulary sample mismatch across dims for dim={d}.\")\n",
    "\n",
    "    Q_train = X[q_idx_train]\n",
    "    Q_test = X[q_idx_test]\n",
    "    Qn_train = Xn[q_idx_train]\n",
    "    Qn_test = Xn[q_idx_test]\n",
    "\n",
    "    gt_ids, gt_scores = ground_truth_topk_cosine(\n",
    "        Xn=Xn,\n",
    "        Qn=Qn_test,\n",
    "        k=TOP_K,\n",
    "        exclude_ids=q_idx_test,      # exclude exact self-match\n",
    "        x_batch=X_BATCH,\n",
    "    )\n",
    "\n",
    "    meta = {\n",
    "        \"dataset_id\": DATASET_ID,\n",
    "        \"glove_path\": str(glove_path),\n",
    "        \"dim\": int(d),\n",
    "        \"N_BASE\": int(N_BASE),\n",
    "        \"NQ_TOTAL\": int(NQ_TOTAL),\n",
    "        \"NQ_TRAIN\": int(q_idx_train.size),\n",
    "        \"NQ_TEST\": int(q_idx_test.size),\n",
    "        \"TOP_K_TEST\": int(TOP_K),\n",
    "        \"SEED\": int(SEED),\n",
    "        \"metric\": \"cosine/ip (unit-normalized)\",\n",
    "        \"base_words_sha256\": master_words_hash,\n",
    "        \"notes\": \"Shared q_idx_train/q_idx_test reused across dimensions; self-match excluded in GT.\",\n",
    "    }\n",
    "\n",
    "    base_out = OUT_DIR / f\"glove{d}d_base_N{N_BASE}__ds{DATASET_ID}.npz\"\n",
    "    queries_out = OUT_DIR / f\"glove{d}d_queries_N{N_BASE}_nq{NQ_TOTAL}_test{NQ_TEST}_seed{SEED}__ds{DATASET_ID}.npz\"\n",
    "    gt_out = OUT_DIR / f\"glove{d}d_gt_test_N{N_BASE}_nq{NQ_TEST}_k{TOP_K}_seed{SEED}__ds{DATASET_ID}.npz\"\n",
    "\n",
    "    np.savez_compressed(\n",
    "        base_out,\n",
    "        dataset_id=np.array(DATASET_ID, dtype=np.str_),\n",
    "        base_words=np.array(base_words, dtype=np.str_),\n",
    "        X=X,\n",
    "        Xn=Xn,\n",
    "        meta=np.array(json.dumps(meta), dtype=np.str_),\n",
    "    )\n",
    "    np.savez_compressed(\n",
    "        queries_out,\n",
    "        dataset_id=np.array(DATASET_ID, dtype=np.str_),\n",
    "        q_idx_all=q_idx_all,\n",
    "        q_idx_train=q_idx_train,\n",
    "        q_idx_test=q_idx_test,\n",
    "        Q_train=Q_train,\n",
    "        Q_test=Q_test,\n",
    "        Qn_train=Qn_train,\n",
    "        Qn_test=Qn_test,\n",
    "        meta=np.array(json.dumps(meta), dtype=np.str_),\n",
    "    )\n",
    "    np.savez_compressed(\n",
    "        gt_out,\n",
    "        dataset_id=np.array(DATASET_ID, dtype=np.str_),\n",
    "        q_idx_test=q_idx_test,\n",
    "        gt_ids=gt_ids,\n",
    "        gt_scores=gt_scores,\n",
    "        meta=np.array(json.dumps(meta), dtype=np.str_),\n",
    "    )\n",
    "\n",
    "    print(\"Saved:\")\n",
    "    print(\"-\", base_out.name)\n",
    "    print(\"-\", queries_out.name)\n",
    "    print(\"-\", gt_out.name)\n",
    "\n",
    "print(\"\\nDone. DATASET_ID:\", DATASET_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b649a46d-e53c-496b-9993-799dea154e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
